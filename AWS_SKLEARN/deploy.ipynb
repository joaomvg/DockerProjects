{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Estimator (Training API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sagemaker.estimator object with\n",
    "* image_uri= local container image name\n",
    "* role: a fake role\n",
    "* instance_count:1\n",
    "* instance_type: 'local', for local mode\n",
    "* sagemaker_session: the local sagemaker_session defined above\n",
    "\n",
    "Container:\n",
    "\n",
    "#base image\n",
    "FROM aws_sklearn0.23\n",
    "\n",
    "RUN pip install sagemaker-containers\n",
    "\n",
    "#Copies the train code inside the container\n",
    "\n",
    "COPY train.py  /opt/ml/code/\n",
    "\n",
    "#Defines train.py as script entry point\n",
    "\n",
    "ENV SAGEMAKER_PROGRAM train.py\n",
    "\n",
    "The sagemaker model will run the script train.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(image_uri='awsklearn',\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='local',sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.deploy(instance_type='local',initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Model (Inference API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sagemaker.model object with\n",
    "\n",
    "* image_uri= local container image name\n",
    "* role: a fake role\n",
    "* sagemaker_session: the local sagemaker_session defined above\n",
    "* model_data: the location of the model file to be loaded\n",
    "\n",
    "Container:\n",
    "\n",
    "#base image\n",
    "FROM aws_sklearn0.23\n",
    "\n",
    "RUN pip install sagemaker-containers\n",
    "\n",
    "#Copies the serve code (and other dependencies if necessary) inside the container\n",
    "\n",
    "COPY serve.py  /opt/ml/code/\n",
    "\n",
    "#Defines serve.py as script entry point\n",
    "\n",
    "ENV SAGEMAKER_PROGRAM serve.py\n",
    "\n",
    "The sagemaker model will run the script train.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(image_uri='awsklearn',\n",
    "                        model_data='file://./MODEL/model.tar.gz',\n",
    "                      role=role,\n",
    "                      sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the short-lived AWS credentials found in session. They might expire while running.\n",
      "Attaching to 1wkiw30gcl-algo-1-h115r\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,507 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,510 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,510 INFO - sagemaker-containers - nginx config: \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m worker_processes auto;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m daemon off;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m error_log  /dev/stderr;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m events {\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m }\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m http {\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   default_type application/octet-stream;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   upstream gunicorn {\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     server unix:/tmp/gunicorn.sock;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   }\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   server {\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     location ~ ^/(ping|invocations|execution-parameters) {\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m       proxy_set_header Host $http_host;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m       proxy_redirect off;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m       proxy_read_timeout 60s;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m       proxy_pass http://gunicorn;\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     }\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     location / {\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m       return 404 \"{}\";\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m     }\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   }\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m }\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,513 INFO - sagemaker-containers - Module serve does not provide a setup.py. \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m Generating setup.py\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,513 INFO - sagemaker-containers - Generating setup.cfg\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,513 INFO - sagemaker-containers - Generating MANIFEST.in\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:45,514 INFO - sagemaker-containers - Installing module with the following command:\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m /miniconda3/bin/python -m pip install . \n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m Building wheels for collected packages: serve\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   Building wheel for serve (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m \u001b[?25h  Created wheel for serve: filename=serve-1.0.0-py2.py3-none-any.whl size=10077 sha256=dfdaefcd9eb2353a73e2e06a2c37991d58dc58192ba6d4de8ffd896f41071d3e\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-h3ummacf/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m Successfully built serve\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021/01/22 15:41:46 [crit] 19#19: *1 connect() to unix:/tmp/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 172.18.0.1, server: , request: \"GET /ping HTTP/1.1\", upstream: \"http://unix:/tmp/gunicorn.sock:/ping\", host: \"localhost:8080\"\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 172.18.0.1 - - [22/Jan/2021:15:41:46 +0000] \"GET /ping HTTP/1.1\" 502 182 \"-\" \"python-urllib3/1.26.2\"\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m Installing collected packages: serve\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m Successfully installed serve-1.0.0\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:46 +0000] [38] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:46 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:46 +0000] [38] [INFO] Using worker: gevent\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:46 +0000] [41] [INFO] Booting worker with pid: 41\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:46 +0000] [42] [INFO] Booting worker with pid: 42\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:47 +0000] [50] [INFO] Booting worker with pid: 50\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:47 +0000] [58] [INFO] Booting worker with pid: 58\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:47 +0000] [59] [INFO] Booting worker with pid: 59\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:47 +0000] [60] [INFO] Booting worker with pid: 60\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:47 +0000] [61] [INFO] Booting worker with pid: 61\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m [2021-01-22 15:41:47 +0000] [62] [INFO] Booting worker with pid: 62\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 2021-01-22 15:41:51,391 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   UserWarning)\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m   UserWarning)\n",
      "!\u001b[36m1wkiw30gcl-algo-1-h115r |\u001b[0m 172.18.0.1 - - [22/Jan/2021:15:41:55 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.2\"\n"
     ]
    }
   ],
   "source": [
    "model.deploy(instance_type='local',initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awsklearn-2021-01-22-15-41-37-547'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the endpoint at\n",
    "http://127.0.0.1:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7e323b1bf0f11bc974555887de159e6dbd244d6d0795bcb793f483469fe5695b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}